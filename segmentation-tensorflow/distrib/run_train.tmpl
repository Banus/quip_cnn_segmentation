#!/bin/bash

<%

tasks = self.i['tasks']

ps_hosts = ''
worker_hosts = ''

for task in tasks:
  if task['job'] == 'worker':
    worker_hosts = '%s,%s:%i' % (worker_hosts, task['addr'], 2222+task['gpu'])
  if task['job'] == 'ps':
    ps_hosts = '%s,%s:%i' % (ps_hosts, task['addr'], 2222+task['gpu'])

ps_hosts = ps_hosts[1:]
worker_hosts = worker_hosts[1:]

ps_task_index = -1
worker_task_index = -1
%>

HOME=/home/jlogan
LE_HOME=/home/lehhou
DISTRIB_DIR=\${HOME}/cnn/quip_cnn_segmentation/segmentation-tensorflow/distrib

export PATH=\${HOME}/cuda-8.0/bin:\${PATH}
export LD_LIBRARY_PATH=\${LE_HOME}/cuda-8.0/lib64:\${LD_LIBRARY_PATH}
export C_INCLUDE_PATH=\${LE_HOME}/cuda-8.0/include:\${C_INCLUDE_PATH}
export CPLUS_INCLUDE_PATH=\${LE_HOME}/cuda-8.0/include:\${CPLUS_INCLUDE_PATH}

#for $task in $tasks
<%
job_name=task['job']
if task['job'] == 'ps':
  ps_task_index += 1
  task_index = ps_task_index
else:
  worker_task_index += 1 
  task_index = worker_task_index

gpu=task['gpu']
%>
ssh ${task['addr']} "cd \${DISTRIB_DIR} && CUDA_VISIBLE_DEVICES=${task['gpu']} \
    LD_LIBRARY_PATH="\${LD_LIBRARY_PATH}:\${LE_HOME}/my_libc_env/lib/x86_64-linux-gnu/:\${LE_HOME}/my_libc_env/usr/lib64/" \
    \${LE_HOME}/my_libc_env/lib/x86_64-linux-gnu/ld-2.17.so \
    ~/anaconda2/bin/python \${DISTRIB_DIR}/main.py --ps_hosts $ps_hosts --worker_hosts $worker_hosts --job_name $job_name --task_index $task_index --gpu $gpu &> log.train.${task['addr']}.txt" &

#end for

wait <%=' '.join(['%{}'.format(i+1) for i in range(2+worker_task_index+ps_task_index)])%>
exit 0
